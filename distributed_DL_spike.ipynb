{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorflow_test():\n",
    "    hello = tf.constant('Hello, TensorFlow!')\n",
    "    sess = tf.Session()\n",
    "    print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test:\n",
    "    def get_data(self):\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "        x_train = x_train.reshape(60000, 784)\n",
    "        x_test = x_test.reshape(10000, 784)\n",
    "        self.y_train = y_train.reshape(60000, 1)\n",
    "        self.y_test = y_test.reshape(10000, 1)\n",
    "        self.x_train = x_train.astype('float32')\n",
    "        self.x_test = x_test.astype('float32')\n",
    "        self.x_train /= 255\n",
    "        self.x_test /= 255\n",
    "        print(self.x_train[0:2].shape)\n",
    "        print(self.y_train.shape)\n",
    "        print(self.x_train[59999])\n",
    "        print(self.y_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 784)\n",
      "(60000, 1)\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.14901961 0.1882353\n",
      " 0.1882353  0.08627451 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.24313726\n",
      " 0.38039216 0.7764706  0.9529412  0.99607843 0.99607843 0.83137256\n",
      " 0.10588235 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.2627451  0.6745098  0.99607843 0.99607843\n",
      " 0.88235295 0.85490197 0.85490197 0.92941177 0.972549   0.15686275\n",
      " 0.         0.08235294 0.6431373  0.73333335 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.34901962\n",
      " 0.85882354 0.99607843 0.38039216 0.2627451  0.05490196 0.\n",
      " 0.         0.36078432 0.90588236 0.47843137 0.09019608 0.79607844\n",
      " 0.9254902  0.23137255 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09803922 0.8509804  0.9490196  0.36078432\n",
      " 0.01568628 0.         0.         0.         0.         0.01568628\n",
      " 0.5764706  0.99215686 0.9411765  0.9098039  0.36078432 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.39607844 1.         0.36078432 0.         0.         0.\n",
      " 0.         0.         0.         0.4117647  0.99607843 0.99607843\n",
      " 0.69411767 0.04313726 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.654902   0.95686275\n",
      " 0.16078432 0.         0.         0.         0.02745098 0.29803923\n",
      " 0.78039217 0.93333334 0.9372549  0.36862746 0.03921569 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.7529412  0.4745098  0.         0.\n",
      " 0.00784314 0.24705882 0.7058824  0.99607843 0.9137255  0.49411765\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.74509805 0.76862746 0.05490196 0.00784314 0.38039216 0.99607843\n",
      " 0.9882353  0.57254905 0.20392157 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.50980395 0.88235295\n",
      " 0.2784314  0.7058824  0.9098039  0.70980394 0.23529412 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.50980395 0.99607843 0.99607843 0.9019608\n",
      " 0.18039216 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.02352941 0.3019608\n",
      " 0.95686275 0.99607843 0.63529414 0.01568628 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.43137255 0.99607843 0.85490197 0.99607843\n",
      " 0.45490196 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.5137255\n",
      " 0.99607843 0.6039216  0.10980392 0.8352941  0.3372549  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.25882354 0.81960785 0.6        0.07450981\n",
      " 0.07450981 0.9137255  0.23529412 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.5568628  0.99607843 0.64705884 0.         0.05490196 0.84705883\n",
      " 0.654902   0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.3529412  0.99607843\n",
      " 0.6862745  0.         0.07058824 0.8980392  0.36078432 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.10196079 0.8980392  0.9764706  0.6901961\n",
      " 0.87058824 0.95686275 0.17254902 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.28627452 0.75686276 0.77254903 0.5254902  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "[[5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [9]]\n"
     ]
    }
   ],
   "source": [
    "t = test()\n",
    "t.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class distributed_model_training:\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tself.num_classes = 10\n",
    "\t\tself.total_num_epochs = 2\n",
    "\t\tself.batch_size = 100\n",
    "\t\tself.get_data()\n",
    "\t\tself.distribute_data()\n",
    "\t\tself.define_models()\n",
    "# \t\tself.train_model_aggregate()\n",
    "\n",
    "\tdef tensorflow_test(self):\n",
    "\t\thello = tf.constant('Hello, TensorFlow!')\n",
    "\t\tsess = tf.Session()\n",
    "\t\tprint(sess.run(hello))\n",
    "\n",
    "\tdef get_data(self):\n",
    "\t\t(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\t\tx_train = x_train.reshape(60000, 784)\n",
    "\t\tx_test = x_test.reshape(10000, 784)\n",
    "\t\tself.y_train = y_train.reshape(60000, 1)\n",
    "\t\tself.y_test = y_test.reshape(10000, 1)\n",
    "\t\tself.x_train = x_train.astype('float32')\n",
    "\t\tself.x_test = x_test.astype('float32')\n",
    "\t\tself.x_train /= 255\n",
    "\t\tself.x_test /= 255\n",
    "\t\t#One-hot encode the y vectors\n",
    "\t\tself.y_train = keras.utils.to_categorical(self.y_train, self.num_classes)\n",
    "\t\tself.y_test = keras.utils.to_categorical(self.y_test, self.num_classes)\n",
    "\n",
    "\tdef distribute_data(self):\n",
    "\t\t#only deal with x_train and y_train\n",
    "\t\t# np.random.shuffle(self.x_train)\n",
    "\t\tself.segment_batches = {}\n",
    "\t\tfor i in range(10):\n",
    "\t\t\tself.segment_batches[\"seg\"+str(i)] = (self.x_train[6000*i:6000*i+6000], self.y_train[6000*i:6000*i+6000])\n",
    "\n",
    "\tdef define_models(self):\n",
    "\t\tself.segment_models = {}\n",
    "\t\tfor i in range(10):\n",
    "\t\t\tmodel = Sequential()\n",
    "\t\t\tmodel.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "\t\t\tmodel.add(Dropout(0.2))\n",
    "\t\t\tmodel.add(Dense(512, activation='relu'))\n",
    "\t\t\tmodel.add(Dropout(0.2))\n",
    "\t\t\tmodel.add(Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "\t\t\tmodel.compile(loss='categorical_crossentropy',\n",
    "\t            optimizer=Adam(),\n",
    "\t            metrics=['accuracy'])\n",
    "\n",
    "\t\t\tself.segment_models[\"seg\"+str(i)] = model\n",
    "\n",
    "\tdef train_model_aggregate(self):\n",
    "\t\tfor i in range(self.total_num_epochs):\n",
    "\t\t\tfor segment in self.segment_models:\n",
    "\t\t\t\t# model_seg = self.segment_models[segment]\n",
    "\t\t\t\t(x_train_seg, y_train_seg) = self.segment_batches[segment]\t\t\t\t\n",
    "\t\t\t\t# self.segment_models[segment] = self.train_model_segment(model_seg, x_train_seg, y_train_seg)\n",
    "\t\t\t\thistory = self.segment_models[segment].fit(x_train_seg, y_train_seg,\n",
    "\t\t\t        batch_size=self.batch_size,\n",
    "\t\t\t        epochs=1,\n",
    "\t\t\t        verbose=0,\n",
    "\t\t\t        validation_data=(self.x_test, self.y_test))\n",
    "\n",
    "\t\t#--------------------------------------------------------------------------------\n",
    "\n",
    "\t\tfor segment in self.segment_models:\n",
    "\t\t\tscore = self.segment_models[segment].evaluate(self.x_test, self.y_test, verbose=0)\n",
    "\t\t\tprint('[Segment ' + str(segment) + '] Test loss: ' + str(score[0]))\n",
    "\t\t\tprint('[Segment ' + str(segment) + '] Test accuracy: ' + str(score[1]))\n",
    "\n",
    "\t\t#--------------------------------------------------------------------------------\n",
    "    \n",
    "\t\taggregate_model = Sequential()\n",
    "\t\taggregate_model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "\t\taggregate_model.add(Dropout(0.2))\n",
    "\t\taggregate_model.add(Dense(512, activation='relu'))\n",
    "\t\taggregate_model.add(Dropout(0.2))\n",
    "\t\taggregate_model.add(Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "\t\taggregate_model.compile(loss='categorical_crossentropy',\n",
    "            optimizer=Adam(),\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "\t\tprint(len(self.segment_models))\n",
    "\n",
    "\t\ta = [self.segment_models[segment].get_weights() for segment in self.segment_models]\n",
    "\t\tprint(type(a[0]))\n",
    "\n",
    "\t\tavg_weights = sum([self.segment_models[segment].get_weights() for segment in self.segment_models])/len(self.segment_models)\n",
    "\t\tprint(avg_weights.shape)\n",
    "\n",
    "\t\t# count = 0\n",
    "\t\t# for segment in self.segment_models:\n",
    "\t\t# \tmodel_seg = self.segment_models[segment]\n",
    "\t\t# \tseg_weights = model_seg.get_weights()\n",
    "\t\t# \tprint(seg_weights.shape)\n",
    "\t\t# \tif not weights:\n",
    "\t\t# \t\tweights = seg_weights\n",
    "\t\t# \t\tcount += 1\n",
    "\t\t# \telse:\n",
    "\t\t# \t\tweights = (weights*count + seg_weights)\n",
    "\t\t# \t\tcount += 1\n",
    "\t\t# \t\tweights /= count \n",
    "\n",
    "\n",
    "\tdef train_model_segment(self, model, x_train, y_train):\n",
    "\t\treturn model\n",
    "\n",
    "\n",
    "model_instance = distributed_model_training()\n",
    "# model_instance.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "a = [np.array(model_instance.segment_models[segment].get_weights()) for segment in model_instance.segment_models]\n",
    "print(type(a[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<type 'numpy.ndarray'>, <type 'numpy.ndarray'>, <type 'numpy.ndarray'>, <type 'numpy.ndarray'>, <type 'numpy.ndarray'>, <type 'numpy.ndarray'>, <type 'numpy.ndarray'>, <type 'numpy.ndarray'>, <type 'numpy.ndarray'>, <type 'numpy.ndarray'>]\n"
     ]
    }
   ],
   "source": [
    "print([type(a[i]) for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([[ 0.01635302, -0.0329359 , -0.13793775, ..., -0.16930667,\n",
       "         0.14292729, -0.09311039],\n",
       "       [-0.06551155,  0.2207056 ,  0.20803404, ...,  0.09243075,\n",
       "         0.0580543 ,  0.15351951],\n",
       "       [ 0.07777649, -0.08474934,  0.20699377, ...,  0.16509902,\n",
       "        -0.06108699, -0.02703522],\n",
       "       ...,\n",
       "       [ 0.18027064,  0.07722271, -0.02912642, ...,  0.01579388,\n",
       "        -0.14919353,  0.07266685],\n",
       "       [ 0.0278835 , -0.05178187,  0.22234823, ...,  0.03538033,\n",
       "         0.10468534,  0.08430828],\n",
       "       [-0.15660337, -0.00164034,  0.14003426, ...,  0.17141129,\n",
       "        -0.14533463, -0.04591301]], dtype=float32),\n",
       "       array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0.], dtype=float32),\n",
       "       array([[-0.06798866, -0.03137019,  0.07286298, ..., -0.01063798,\n",
       "        -0.17529961,  0.09505405],\n",
       "       [-0.02597789,  0.02698177,  0.02186535, ..., -0.05252092,\n",
       "        -0.08227771, -0.01624758],\n",
       "       [ 0.08479075,  0.23758635, -0.27357814, ...,  0.110245  ,\n",
       "        -0.13690834, -0.04368763],\n",
       "       ...,\n",
       "       [-0.18492354, -0.20019783, -0.15388122, ...,  0.01010425,\n",
       "         0.13647711,  0.18306729],\n",
       "       [ 0.11528003,  0.13238187, -0.10179676, ...,  0.10373145,\n",
       "         0.08862649, -0.11113181],\n",
       "       [ 0.02089493, -0.05187588, -0.2941908 , ...,  0.16340613,\n",
       "        -0.1011773 ,  0.28668433]], dtype=float32),\n",
       "       array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0.], dtype=float32),\n",
       "       array([[-0.10424955, -0.22968267, -0.17857209, ..., -0.11155278,\n",
       "        -0.10733891,  0.16706347],\n",
       "       [-0.04062996, -0.24617487, -0.35683304, ...,  0.53117865,\n",
       "        -0.3481514 , -0.09499276],\n",
       "       [ 0.18941921, -0.08262872,  0.01841244, ..., -0.3747463 ,\n",
       "        -0.09493193, -0.40693024],\n",
       "       ...,\n",
       "       [-0.04229295,  0.08585428, -0.18531016, ..., -0.00104317,\n",
       "        -0.02910847,  0.01548297],\n",
       "       [-0.38014692,  0.14687753, -0.03879733, ...,  0.00917817,\n",
       "         0.31494543, -0.07302658],\n",
       "       [ 0.20451489,  0.05554677,  0.11191289, ...,  0.06886458,\n",
       "         0.05590696, -0.06325907]], dtype=float32),\n",
       "       array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['seg0', 'seg1', 'seg2', 'seg3', 'seg4', 'seg5', 'seg6', 'seg7', 'seg8', 'seg9']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(model_instance.segment_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(784, 512), (512,), (512, 512), (512,), (512, 10), (10,)]\n"
     ]
    }
   ],
   "source": [
    "aggregate_model = Sequential()\n",
    "aggregate_model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "aggregate_model.add(Dropout(0.2))\n",
    "aggregate_model.add(Dense(512, activation='relu'))\n",
    "aggregate_model.add(Dropout(0.2))\n",
    "aggregate_model.add(Dense(10, activation='softmax'))\n",
    "print([i.shape for i in aggregate_model.get_weights()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<matplotlib.mlab.PCA instance at 0x11345de18>\n"
     ]
    }
   ],
   "source": [
    "from matplotlib.mlab import PCA\n",
    "import numpy as np\n",
    "data = np.array(np.random.randint(10,size=(10,3)))\n",
    "results = PCA(data)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(data, dims_rescaled_data=2):\n",
    "    \"\"\"\n",
    "    returns: data transformed in 2 dims/columns + regenerated original data\n",
    "    pass in: data as 2D NumPy array\n",
    "    \"\"\"\n",
    "    import numpy as NP\n",
    "    from scipy import linalg as LA\n",
    "    m, n = data.shape\n",
    "    # mean center the data\n",
    "    data -= data.mean(axis=0)\n",
    "    # calculate the covariance matrix\n",
    "    R = NP.cov(data, rowvar=False)\n",
    "    # calculate eigenvectors & eigenvalues of the covariance matrix\n",
    "    # use 'eigh' rather than 'eig' since R is symmetric, \n",
    "    # the performance gain is substantial\n",
    "    evals, evecs = LA.eigh(R)\n",
    "    # sort eigenvalue in decreasing order\n",
    "    idx = NP.argsort(evals)[::-1]\n",
    "    evecs = evecs[:,idx]\n",
    "    # sort eigenvectors according to same index\n",
    "    evals = evals[idx]\n",
    "    # select the first n eigenvectors (n is desired dimension\n",
    "    # of rescaled data array, or dims_rescaled_data)\n",
    "    evecs = evecs[:, :dims_rescaled_data]\n",
    "    # carry out the transformation on the data using eigenvectors\n",
    "    # and return the re-scaled data, eigenvalues, and eigenvectors\n",
    "    return NP.dot(evecs.T, data.T).T, evals, evecs\n",
    "\n",
    "def test_PCA(data, dims_rescaled_data=2):\n",
    "    '''\n",
    "    test by attempting to recover original data array from\n",
    "    the eigenvectors of its covariance matrix & comparing that\n",
    "    'recovered' array with the original data\n",
    "    '''\n",
    "    _ , _ , eigenvectors = PCA(data, dim_rescaled_data=2)\n",
    "    data_recovered = NP.dot(eigenvectors, m).T\n",
    "    data_recovered += data_recovered.mean(axis=0)\n",
    "    assert NP.allclose(data, data_recovered)\n",
    "\n",
    "\n",
    "def plot_pca(data):\n",
    "    from matplotlib import pyplot as MPL\n",
    "    clr1 =  '#2026B2'\n",
    "    fig = MPL.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    data_resc, data_orig = PCA(data)\n",
    "    ax1.plot(data_resc[:, 0], data_resc[:, 1], '.', mfc=clr1, mec=clr1)\n",
    "    MPL.show()\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
