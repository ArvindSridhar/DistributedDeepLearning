Clustering of weights: local minima (throw away the rest of the clusters if one is clearly best or merge clusters and try to re-train)
Run more iterations between merging

---------------------------------------------------------------------------------------------------------------------------------------

Data distribution problem
-Idea 1: have each segment get all data, run diff. epochs on diff. segments to parallelize that way (kinda defeats the purpose tho)
-Fundamentally it's a data problem: each segment does not have enough data to train adequately and find the right minima/optima, so weights are too dissimilar and averaging can't be done reliably
-Data augmentation is a solution? Flip each image across vertical axis, that doubles your dataset on each segment
-NOPE: clearly this is a wrong assumption, there is enough data on every segment, more data creates bad results, the spatial distribution idea is much more promising

---------------------------------------------------------------------------------------------------------------------------------------

#2 ideas: 1) add lotsa data to each segment, train 10 segments, merge, see what happens; 2) figure out how to average better to achieve better spatial distribution in plot

So the problem seems to be with the scale of the weights after averaging: they are scaled down, which significantly impacts performance
-Scaling the weights down reduces accuracy, while scaling the weights up increases accuracy (it's a direct correlation)

---Update---
The lotsa data to a segment idea is definitely not correct
-Trained each segment with all the data, achieved 11% accuracy, obviously not what we want
-Data to a segment is fine, the problem is weights that are too dissimilar between large numbers of segments, averaging cannot be done reliably

So the problem is with number of segments, not data alloted to each segment, as previously conjectured

Spatial distribution seems promising

Need smarter way to merge: intelligent merging of the weights
-Autoencoder?
-Neural net?
-We must judge the output of the neural network: will output weights that we can then use on test set, must meet a threshold, that's how you train


---------------------------------------------------------------------------------------------------------------------------------------

Merging strategies

Order them by loss on test set
Compute weighted average with predetermined weightiness: first one gets 50% of say, second one 25%, third one 12.5%, fourth 6.25%, etc., last one gets rest of say


2 segments: it merges nicely and maintains same accuracy because the merged weights still closely resemble the weights of both segments

10 segments: it doesn’t merge nicely because the merged weights are averages of a whole bunch of segments, at the end doesn’t look like any of them

---------------------------------------------------------------------------------------------------------------------------------------

Is merging models to create a final single model actually worth it?

Can treat it like random forest: have n diff. models from each of n segments, get a piece of test data, have each model vote on classification/regression, output the consensus vote










# Maybe add aggregate model prediction to consensus as a tiebreaker
# have each model initialized at the same starting point, and then have it run for a few iterations so that they aren't too divergent
# output the results of the model to file
# progressively increase the number of iterations with each grand epoch
#Instead of blind consensus prediction, pick using intelligent strategies, like if model succeeded on this example before during training or something, obviously pick it (or a similar example, gauge similarity betw. examples), so a weighted consensus












#Think in terms of a single val example: input this 1 by 784 row vector (one image),
#each model outputs a prediction, predictions multiplied by weights, softmaxed at the end to get final prediction

#Convolutional layer run over the 2D array output that is ensemble_predictions? Maybe run convolution layer over
#the actual segment models themselves, over their weight vectors, to get compression?
#-Nope, because this is really just a collection of training examples, can't really do this

#You can run the training, val, and test data on this, but want to default to using the test data (last 5000 examples)